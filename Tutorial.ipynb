{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "import torchtext\n",
    "from IPython.display import display\n",
    "from trainer.supervised_trainer import SupervisedTrainer\n",
    "from models.seq2seq import EncoderRNN, DecoderRNN, Seq2seq\n",
    "from seq2seq.utils import SourceField, TargetField\n",
    "from seq2seq.optim import Optimizer\n",
    "from seq2seq.loss import Perplexity\n",
    "from seq2seq.evaluator import Predictor\n",
    "from torchtext.data import Field\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from torchtext.data import TabularDataset\n",
    "from seq2seq.utils import Checkpoint\n",
    "import csv\n",
    "# import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A property in Yeovil has been slapped with a c...</td>\n",
       "      <td>Police slap Yeovil property on Fosse Park Road...</td>\n",
       "      <td>404847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Focus Features/Sony Pictures Classics Gary Old...</td>\n",
       "      <td>2018 Oscars: Can Timothee Chalamet pull off up...</td>\n",
       "      <td>404848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  A property in Yeovil has been slapped with a c...   \n",
       "1  Focus Features/Sony Pictures Classics Gary Old...   \n",
       "\n",
       "                                               title      id  \n",
       "0  Police slap Yeovil property on Fosse Park Road...  404847  \n",
       "1  2018 Oscars: Can Timothee Chalamet pull off up...  404848  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../data/'\n",
    "# file_name = 'train_data.csv'\n",
    "dev_name = 'val_data.csv'\n",
    "val_data = pd.read_csv(os.path.join(data_dir, dev_name), encoding='utf-8')\n",
    "display(val_data.head(n=2))\n",
    "csv.field_size_limit(100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(sentences):\n",
    "    sentences = sentences.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentences)\n",
    "    filtered_words = [w for w in tokens]\n",
    "    return filtered_words\n",
    "\n",
    "max_encoder_len = 800\n",
    "min_decoder_len = 1\n",
    "content, title = SourceField(tokenize=tokenizer), TargetField(tokenize=tokenizer)\n",
    "def len_filter(example):\n",
    "    return len(example.content) <= max_encoder_len and len(example.title) >= min_decoder_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tv_datafields = [('content', content), ('title', title), ('id', None)]  # must order the data format with the csv file.\n",
    "# trn = TabularDataset(path=os.path.join(data_dir, file_name), \n",
    "#                      format='csv', fields=tv_datafields, skip_header=True,\n",
    "#                      filter_pred=len_filter)\n",
    "\n",
    "dev = TabularDataset(path=os.path.join(data_dir, dev_name),\n",
    "                    format='csv', fields = tv_datafields, skip_header=True,\n",
    "                    filter_pred=len_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.build_vocab(dev, max_size = 50000)\n",
    "title.build_vocab(dev, max_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 377922),\n",
       " ('to', 194328),\n",
       " ('and', 168313),\n",
       " ('a', 157451),\n",
       " ('of', 153595),\n",
       " ('in', 132970),\n",
       " ('s', 82589),\n",
       " ('for', 71629),\n",
       " ('that', 71158),\n",
       " ('is', 64632)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('<sos>', 16625),\n",
       " ('<eos>', 16625),\n",
       " ('to', 4879),\n",
       " ('the', 3629),\n",
       " ('in', 3310),\n",
       " ('s', 3285),\n",
       " ('of', 2753),\n",
       " ('for', 2532),\n",
       " ('a', 1957),\n",
       " ('and', 1853)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(content.vocab.freqs.most_common(10))\n",
    "display(title.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_vocab = content.vocab\n",
    "output_vocab = title.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model here.\n",
    "weight = torch.ones(len(title.vocab))\n",
    "pad = title.vocab.stoi[title.pad_token]\n",
    "loss = Perplexity(weight, pad)\n",
    "# loss.cuda()\n",
    "seq2seq = None\n",
    "optimizer = None\n",
    "hidden_size = 100\n",
    "bidirectional = True\n",
    "# add the pretrained embedding here\n",
    "# encoder_embedding = torch.from_numpy(np.load('../data/encoder_embedding_50000_100.npy'))\n",
    "# decoder_embedding = torch.from_numpy(np.load('../data/decoder_embedding_50000_100.npy'))\n",
    "# display(encoder_embedding.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(len(content.vocab), max_encoder_len, hidden_size, bidirectional=bidirectional, dropout_p=0.2, n_layers=2,\n",
    "                     variable_lengths=True, update_embedding=True)\n",
    "decoder = DecoderRNN(len(title.vocab), 20, hidden_size*2 if bidirectional else hidden_size, dropout_p=0.2, n_layers=2, use_attention=True, \n",
    "                     bidirectional=bidirectional, eos_id = title.eos_id, sos_id = title.sos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seq2seq =Seq2seq(encoder, decoder)\n",
    "# my_seq2seq.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in my_seq2seq.parameters():\n",
    "    param.data.uniform_(-0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = SupervisedTrainer(loss = loss, batch_size=40, checkpoint_every=3e4, print_every=100, expt_dir='../data', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "my_seq2seq = t.train(my_seq2seq, dev, num_epochs=2, optimizer=optimizer, teacher_forcing_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Predictor(object):\n",
    "\n",
    "    def __init__(self, model, src_vocab, tgt_vocab):\n",
    "        \"\"\"\n",
    "        Predictor class to evaluate for a given model.\n",
    "        Args:\n",
    "            model (seq2seq.models): trained model. This can be loaded from a checkpoint\n",
    "                using `seq2seq.util.checkpoint.load`\n",
    "            src_vocab (seq2seq.dataset.vocabulary.Vocabulary): source sequence vocabulary\n",
    "            tgt_vocab (seq2seq.dataset.vocabulary.Vocabulary): target sequence vocabulary\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = model.cuda()\n",
    "        else:\n",
    "            self.model = model.cpu()\n",
    "        self.model.eval()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def get_decoder_features(self, src_seq):\n",
    "        src_id_seq = torch.LongTensor([self.src_vocab.stoi[tok] for tok in src_seq]).view(1, -1)\n",
    "        if torch.cuda.is_available():\n",
    "            src_id_seq = src_id_seq.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            softmax_list, _, other = self.model(src_id_seq, [len(src_seq)])\n",
    "\n",
    "        return other\n",
    "\n",
    "    def predict(self, src_seq):\n",
    "        \"\"\" Make prediction given `src_seq` as input.\n",
    "\n",
    "        Args:\n",
    "            src_seq (list): list of tokens in source language\n",
    "\n",
    "        Returns:\n",
    "            tgt_seq (list): list of tokens in target language as predicted\n",
    "            by the pre-trained model\n",
    "        \"\"\"\n",
    "        other = self.get_decoder_features(src_seq)\n",
    "\n",
    "        length = other['length'][0]\n",
    "\n",
    "        tgt_id_seq = [other['sequence'][di][0].data[0] for di in range(length)]\n",
    "        tgt_seq = [self.tgt_vocab.itos[tok] for tok in tgt_id_seq]\n",
    "        return tgt_seq\n",
    "\n",
    "    def predict_n(self, src_seq, n=1):\n",
    "        \"\"\" Make 'n' predictions given `src_seq` as input.\n",
    "\n",
    "        Args:\n",
    "            src_seq (list): list of tokens in source language\n",
    "            n (int): number of predicted seqs to return. If None,\n",
    "                     it will return just one seq.\n",
    "\n",
    "        Returns:\n",
    "            tgt_seq (list): list of tokens in target language as predicted\n",
    "                            by the pre-trained model\n",
    "        \"\"\"\n",
    "        other = self.get_decoder_features(src_seq)\n",
    "\n",
    "        result = []\n",
    "        for x in range(0, int(n)):\n",
    "            length = other['topk_length'][0][x]\n",
    "            tgt_id_seq = [other['topk_sequence'][di][0, x, 0].data[0] for di in range(length)]\n",
    "            tgt_seq = [self.tgt_vocab.itos[tok] for tok in tgt_id_seq]\n",
    "            result.append(tgt_seq)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicor = Predictor(my_seq2seq, input_vocab, output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/test_data.csv', encoding='utf-8')\n",
    "test_contents = list(test_data.content)\n",
    "# test_titles = list(test_data.title)[:20]\n",
    "# just use the former 20 sets as the \n",
    "test_contents =  [tokenizer(content) for content in test_contents]\n",
    "test_results = []\n",
    "for index, content_ in enumerate(test_contents):\n",
    "    test_title = ' '.join(predicor.predict(content_))\n",
    "    test_results.append(test_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how a mom is murder in a crime <eos>\n",
      "liverpool fans have liverpool ever ever liverpool <eos>\n",
      "these people are people are to to <eos>\n",
      "you are the most you you <eos>\n",
      "the are are the <eos>\n",
      "what 5 things you should know about <eos>\n",
      "japanese s abe is a s s s s s in <eos>\n",
      "amazon s new is new with with with <eos>\n",
      "there s a flight for the delta and to <eos>\n",
      "23 years now now you get free serving you <eos>\n",
      "gordon ramsay is vegan vegan vegan to to to <eos>\n",
      "krispy kreme is kreme kreme kreme kreme kreme kreme kreme <eos>\n",
      "get a out sized bag packs 20 percent of a m s <eos>\n",
      "internet starbucks launches starbucks starbucks clothing store <eos>\n",
      "amazon s amazon to a delivery <eos>\n",
      "<unk> 5 5 to <eos>\n",
      "the 18 day of <eos>\n",
      "khloé kardashian s her khloé her her her her her <eos>\n",
      "the best spot for the weirdest camera videos <eos>\n",
      "the funniest craziest cats you hair in your red cats <eos>\n",
      "the dog dog dog dog dog dog with <eos>\n"
     ]
    }
   ],
   "source": [
    "for index, title_ in enumerate(test_results):\n",
    "    print(title_)\n",
    "    if index == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how a mom is murder in a crime\n",
      "liverpool fans have liverpool ever ever liverpool\n",
      "these people are people are to to\n",
      "you are the most you you\n",
      "the are are the\n",
      "what 5 things you should know about\n",
      "japanese s abe is a s s s s s in\n",
      "amazon s new is new with with with\n",
      "there s a flight for the delta and to\n",
      "23 years now now you get free serving you\n",
      "gordon ramsay is vegan vegan vegan to to to\n",
      "krispy kreme is kreme kreme kreme kreme kreme kreme kreme\n",
      "get a out sized bag packs 20 percent of a m s\n",
      "internet starbucks launches starbucks starbucks clothing store\n",
      "amazon s amazon to a delivery\n",
      "<unk> 5 5 to\n",
      "the 18 day of\n",
      "khloé kardashian s her khloé her her her her her\n",
      "the best spot for the weirdest camera videos\n",
      "the funniest craziest cats you hair in your red cats\n",
      "the dog dog dog dog dog dog with\n"
     ]
    }
   ],
   "source": [
    "test_results = [item[:-6] for item in test_results]\n",
    "for index, title_ in enumerate(test_results):\n",
    "    print(title_)\n",
    "    if index == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_dir = '../data/result'\n",
    "if not os.path.exists(test_data_dir):\n",
    "    os.mkdir(test_data_dir)\n",
    "else:\n",
    "    shutil.rmtree(test_data_dir)\n",
    "    os.mkdir(test_data_dir)\n",
    "\n",
    "for i in range(len(test_results)):\n",
    "    with open(os.path.join(test_data_dir, str(i+1)+'.txt'), 'w') as f:\n",
    "        f.write(test_results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x7f92fbe29cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x7f92fbe29cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to check the unk and pad token in the \n",
    "display(content.vocab)\n",
    "display(title.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(title.vocab))  # the len method will drop the eos and sos token in the sentneces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the pretrainde model here to initialize the embedding matrix here.\n",
    "embedding_matrix = np.zeros((len(content.vocab), 100))\n",
    "word_to_vec_path = '../data/glove.6B.100d.txt'\n",
    "def get_eng_vec(path= word_to_vec_path):\n",
    "    word_to_vec = dict()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line=line.split(' ')\n",
    "            word_to_vec[line[0]]= [float(f) for f in line[1:]]\n",
    "    return word_to_vec\n",
    "\n",
    "word_to_vec = get_eng_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find unknow 0 words in word2vec\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.random.randn(len(title.vocab), 100)\n",
    "for index in range(embedding_matrix.shape[0]):\n",
    "    unknow_words = 0\n",
    "    word = title.vocab.itos[index]\n",
    "    try: # try to find the word in the word_to_vec:\n",
    "        vector = word_to_vec[word]\n",
    "        embedding_matrix[index] = vector\n",
    "    except KeyError:\n",
    "        unknow_words += 1\n",
    "        pass\n",
    "print(\"find unknow {} words in word2vec\".format(unknow_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[-0.1897,\n",
       " 0.050024,\n",
       " 0.19084,\n",
       " -0.049184,\n",
       " -0.089737,\n",
       " 0.21006,\n",
       " -0.54952,\n",
       " 0.098377,\n",
       " -0.20135,\n",
       " 0.34241,\n",
       " -0.092677,\n",
       " 0.161,\n",
       " -0.13268,\n",
       " -0.2816,\n",
       " 0.18737,\n",
       " -0.42959,\n",
       " 0.96039,\n",
       " 0.13972,\n",
       " -1.0781,\n",
       " 0.40518,\n",
       " 0.50539,\n",
       " -0.55064,\n",
       " 0.4844,\n",
       " 0.38044,\n",
       " -0.0029055,\n",
       " -0.34942,\n",
       " -0.099696,\n",
       " -0.78368,\n",
       " 1.0363,\n",
       " -0.2314,\n",
       " -0.47121,\n",
       " 0.57126,\n",
       " -0.21454,\n",
       " 0.35958,\n",
       " -0.48319,\n",
       " 1.0875,\n",
       " 0.28524,\n",
       " 0.12447,\n",
       " -0.039248,\n",
       " -0.076732,\n",
       " -0.76343,\n",
       " -0.32409,\n",
       " -0.5749,\n",
       " -1.0893,\n",
       " -0.41811,\n",
       " 0.4512,\n",
       " 0.12112,\n",
       " -0.51367,\n",
       " -0.13349,\n",
       " -1.1378,\n",
       " -0.28768,\n",
       " 0.16774,\n",
       " 0.55804,\n",
       " 1.5387,\n",
       " 0.018859,\n",
       " -2.9721,\n",
       " -0.24216,\n",
       " -0.92495,\n",
       " 2.1992,\n",
       " 0.28234,\n",
       " -0.3478,\n",
       " 0.51621,\n",
       " -0.43387,\n",
       " 0.36852,\n",
       " 0.74573,\n",
       " 0.072102,\n",
       " 0.27931,\n",
       " 0.92569,\n",
       " -0.050336,\n",
       " -0.85856,\n",
       " -0.1358,\n",
       " -0.92551,\n",
       " -0.33991,\n",
       " -1.0394,\n",
       " -0.067203,\n",
       " -0.21379,\n",
       " -0.4769,\n",
       " 0.21377,\n",
       " -0.84008,\n",
       " 0.052536,\n",
       " 0.59298,\n",
       " 0.29604,\n",
       " -0.67644,\n",
       " 0.13916,\n",
       " -1.5504,\n",
       " -0.20765,\n",
       " 0.7222,\n",
       " 0.52056,\n",
       " -0.076221,\n",
       " -0.15194,\n",
       " -0.13134,\n",
       " 0.058617,\n",
       " -0.31869,\n",
       " -0.61419,\n",
       " -0.62393,\n",
       " -0.41548,\n",
       " -0.038175,\n",
       " -0.39804,\n",
       " 0.47647,\n",
       " -0.15983]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([-1.8970e-01,  5.0024e-02,  1.9084e-01, -4.9184e-02, -8.9737e-02,\n",
       "        2.1006e-01, -5.4952e-01,  9.8377e-02, -2.0135e-01,  3.4241e-01,\n",
       "       -9.2677e-02,  1.6100e-01, -1.3268e-01, -2.8160e-01,  1.8737e-01,\n",
       "       -4.2959e-01,  9.6039e-01,  1.3972e-01, -1.0781e+00,  4.0518e-01,\n",
       "        5.0539e-01, -5.5064e-01,  4.8440e-01,  3.8044e-01, -2.9055e-03,\n",
       "       -3.4942e-01, -9.9696e-02, -7.8368e-01,  1.0363e+00, -2.3140e-01,\n",
       "       -4.7121e-01,  5.7126e-01, -2.1454e-01,  3.5958e-01, -4.8319e-01,\n",
       "        1.0875e+00,  2.8524e-01,  1.2447e-01, -3.9248e-02, -7.6732e-02,\n",
       "       -7.6343e-01, -3.2409e-01, -5.7490e-01, -1.0893e+00, -4.1811e-01,\n",
       "        4.5120e-01,  1.2112e-01, -5.1367e-01, -1.3349e-01, -1.1378e+00,\n",
       "       -2.8768e-01,  1.6774e-01,  5.5804e-01,  1.5387e+00,  1.8859e-02,\n",
       "       -2.9721e+00, -2.4216e-01, -9.2495e-01,  2.1992e+00,  2.8234e-01,\n",
       "       -3.4780e-01,  5.1621e-01, -4.3387e-01,  3.6852e-01,  7.4573e-01,\n",
       "        7.2102e-02,  2.7931e-01,  9.2569e-01, -5.0336e-02, -8.5856e-01,\n",
       "       -1.3580e-01, -9.2551e-01, -3.3991e-01, -1.0394e+00, -6.7203e-02,\n",
       "       -2.1379e-01, -4.7690e-01,  2.1377e-01, -8.4008e-01,  5.2536e-02,\n",
       "        5.9298e-01,  2.9604e-01, -6.7644e-01,  1.3916e-01, -1.5504e+00,\n",
       "       -2.0765e-01,  7.2220e-01,  5.2056e-01, -7.6221e-02, -1.5194e-01,\n",
       "       -1.3134e-01,  5.8617e-02, -3.1869e-01, -6.1419e-01, -6.2393e-01,\n",
       "       -4.1548e-01, -3.8175e-02, -3.9804e-01,  4.7647e-01, -1.5983e-01])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(title.vocab.stoi['to'])\n",
    "vector = word_to_vec['to']\n",
    "display(vector)\n",
    "display(embedding_matrix[4,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../data/decoder_embedding_50000_100.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor_embedding = torch.from_numpy(embedding_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
