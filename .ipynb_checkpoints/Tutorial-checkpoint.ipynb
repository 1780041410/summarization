{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenxiang/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "import torchtext\n",
    "from IPython.display import display\n",
    "from trainer.supervised_trainer import SupervisedTrainer\n",
    "from models.seq2seq import EncoderRNN, DecoderRNN, Seq2seq\n",
    "from seq2seq.utils import SourceField, TargetField\n",
    "from seq2seq.optim import Optimizer\n",
    "from seq2seq.loss import Perplexity\n",
    "from seq2seq.evaluator import Predictor\n",
    "from torchtext.data import Field\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from torchtext.data import TabularDataset\n",
    "from seq2seq.utils import Checkpoint\n",
    "import csv\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was the year that felt like 50 years. We ...</td>\n",
       "      <td>21 Stories Our Readers Loved in 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gary Vaynerchuk once told a 20 year old Taylor...</td>\n",
       "      <td>What To Do After Graduating College</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  This was the year that felt like 50 years. We ...   \n",
       "1  Gary Vaynerchuk once told a 20 year old Taylor...   \n",
       "\n",
       "                                  title  id  \n",
       "0  21 Stories Our Readers Loved in 2017   0  \n",
       "1   What To Do After Graduating College   1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../data/'\n",
    "file_name = 'train_data.csv'\n",
    "dev_name = 'val_data.csv'\n",
    "train_data = pd.read_csv(os.path.join(data_dir, file_name), encoding='utf-8')\n",
    "display(train_data.head(n=2))\n",
    "csv.field_size_limit(100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = \"0123456789\"\n",
    "table = {ord(char): None for char in to_remove}\n",
    "def tokenizer(sentences):\n",
    "    sentences = sentences.lower()\n",
    "#     sentences = sentences.translate(table)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentences)\n",
    "    filtered_words = [w for w in tokens]\n",
    "    return filtered_words\n",
    "\n",
    "max_encoder_len = 800\n",
    "min_decoder_len = 1\n",
    "content, title = SourceField(tokenize=tokenizer), TargetField(tokenize=tokenizer)\n",
    "def len_filter(example):\n",
    "    return len(example.content) <= max_encoder_len and len(example.title) >= min_decoder_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 8s, sys: 20.1 s, total: 3min 28s\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tv_datafields = [('content', content), ('title', title), ('id', None)]  # must order the data format with the csv file.\n",
    "trn = TabularDataset(path=os.path.join(data_dir, file_name), \n",
    "                     format='csv', fields=tv_datafields, skip_header=True,\n",
    "                     filter_pred=len_filter)\n",
    "\n",
    "dev = TabularDataset(path=os.path.join(data_dir, dev_name),\n",
    "                    format='csv', fields = tv_datafields, skip_header=True,\n",
    "                    filter_pred=len_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.build_vocab(trn, max_size = 50000)\n",
    "title.build_vocab(dev, max_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 7215050),\n",
       " ('to', 3798408),\n",
       " ('and', 3324064),\n",
       " ('a', 3130495),\n",
       " ('of', 2999731),\n",
       " ('in', 2567508),\n",
       " ('s', 1543622),\n",
       " ('that', 1397337),\n",
       " ('for', 1383377),\n",
       " ('is', 1297595)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('<sos>', 16625),\n",
       " ('<eos>', 16625),\n",
       " ('to', 4879),\n",
       " ('the', 3629),\n",
       " ('in', 3310),\n",
       " ('s', 3285),\n",
       " ('of', 2753),\n",
       " ('for', 2532),\n",
       " ('a', 1957),\n",
       " ('and', 1853)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(content.vocab.freqs.most_common(10))\n",
    "display(title.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab = content.vocab\n",
    "output_vocab = title.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**建立预训练的embedding matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50002, 200)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find 2446 unknow words in encoder vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20002, 200)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find 1186 unknow words in word2vec\n"
     ]
    }
   ],
   "source": [
    "# load the pretrainde model here to initialize the embedding matrix here.\n",
    "word_to_vec_path = '../data/embedding_matrix/glove/glove.6B.200d.txt'\n",
    "def get_eng_vec(path= word_to_vec_path):\n",
    "    word_to_vec = dict()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line=line.split(' ')\n",
    "            word_to_vec[line[0]]= [float(f) for f in line[1:]]\n",
    "    return word_to_vec\n",
    "\n",
    "word_to_vec = get_eng_vec()\n",
    "\n",
    "encoder_embedding_matrix = np.random.randn(len(content.vocab), 200)\n",
    "display(encoder_embedding_matrix.shape)\n",
    "unknow_words = []\n",
    "for index in range(encoder_embedding_matrix.shape[0]):\n",
    "    word = content.vocab.itos[index]\n",
    "    try:\n",
    "        vector = word_to_vec[word]\n",
    "        encoder_embedding_matrix[index, ] = vector\n",
    "    except KeyError:\n",
    "        unknow_words.append(word)\n",
    "print(\"find {} unknow words in encoder vocab\".format(len(unknow_words)))\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "decoder_embedding_matrix = np.random.randn(len(title.vocab), 200)\n",
    "display(decoder_embedding_matrix.shape)\n",
    "unknow_words = 0\n",
    "for index in range(decoder_embedding_matrix.shape[0]):\n",
    "    word = title.vocab.itos[index]\n",
    "    try: # try to find the word in the word_to_vec:\n",
    "        vector = word_to_vec[word]\n",
    "        decoder_embedding_matrix[index, ] = vector\n",
    "    except KeyError:\n",
    "        unknow_words += 1\n",
    "        pass\n",
    "print(\"find {} unknow words in word2vec\".format(unknow_words))\n",
    "\n",
    "np.save('../data/embedding_matrix/encoder_embedding_{}_200.npy'.format(len(content.vocab)), encoder_embedding_matrix)\n",
    "np.save('../data/embedding_matrix/decoder_embedding_{}_200.npy'.format(len(title.vocab)),decoder_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20002, 200])\n",
      "20002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenxiang/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# build the model here.\n",
    "weight = torch.ones(len(title.vocab))\n",
    "pad = title.vocab.stoi[title.pad_token]\n",
    "loss = Perplexity(weight, pad)\n",
    "seq2seq = None\n",
    "optimizer = None\n",
    "hidden_size = 100\n",
    "bidirectional = True\n",
    "embedding_dim = 200\n",
    "# must notice to convert the embedding matrix to float32, by default, numpy\n",
    "# just convert the data as float64, which is double format\n",
    "encoder_embedding_matrix = torch.from_numpy(np.load('../data/embedding_matrix/encoder_embedding_50002_200.npy').astype('float32'))\n",
    "decoder_embedding_matrix = torch.from_numpy(np.load('../data/embedding_matrix/decoder_embedding_20002_200.npy').astype('float32'))\n",
    "print(decoder_embedding_matrix.shape)\n",
    "print(len(output_vocab))\n",
    "# add the pretrained embedding here\n",
    "# encoder_embedding = torch.from_numpy(np.load('../data/encoder_embedding_50000_100.npy'))\n",
    "# decoder_embedding = torch.from_numpy(np.load('../data/decoder_embedding_50000_100.npy'))\n",
    "# display(encoder_embedding.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(len(content.vocab), max_encoder_len, hidden_size, bidirectional=bidirectional, \n",
    "                     dropout_p=0.2, n_layers=2, variable_lengths=True,embedding=encoder_embedding_matrix,\n",
    "                     embedding_dim = embedding_dim, update_embedding=True)\n",
    "decoder = DecoderRNN(len(title.vocab), 20, embedding_dim, hidden_size*2 if bidirectional else hidden_size, dropout_p=0.2, n_layers=2, use_attention=True, \n",
    "                     bidirectional=bidirectional, eos_id = title.eos_id, sos_id = title.sos_id, embedding=decoder_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.cuda()\n",
    "my_seq2seq =Seq2seq(encoder, decoder)\n",
    "my_seq2seq.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in my_seq2seq.parameters():\n",
    "    param.data.uniform_(-0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 0 to use the GPU:0\n",
    "t = SupervisedTrainer(loss = loss, batch_size=40, checkpoint_every=3e4, print_every=100, expt_dir='../data', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "my_seq2seq = t.train(my_seq2seq, trn, num_epochs=2, optimizer=optimizer, teacher_forcing_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Predictor(object):\n",
    "\n",
    "    def __init__(self, model, src_vocab, tgt_vocab):\n",
    "        \"\"\"\n",
    "        Predictor class to evaluate for a given model.\n",
    "        Args:\n",
    "            model (seq2seq.models): trained model. This can be loaded from a checkpoint\n",
    "                using `seq2seq.util.checkpoint.load`\n",
    "            src_vocab (seq2seq.dataset.vocabulary.Vocabulary): source sequence vocabulary\n",
    "            tgt_vocab (seq2seq.dataset.vocabulary.Vocabulary): target sequence vocabulary\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = model.cuda()\n",
    "        else:\n",
    "            self.model = model.cpu()\n",
    "        self.model.eval()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def get_decoder_features(self, src_seq):\n",
    "        src_id_seq = torch.LongTensor([self.src_vocab.stoi[tok] for tok in src_seq]).view(1, -1)\n",
    "        if torch.cuda.is_available():\n",
    "            src_id_seq = src_id_seq.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            softmax_list, _, other = self.model(src_id_seq, [len(src_seq)])\n",
    "\n",
    "        return other\n",
    "\n",
    "    def predict(self, src_seq):\n",
    "        \"\"\" Make prediction given `src_seq` as input.\n",
    "\n",
    "        Args:\n",
    "            src_seq (list): list of tokens in source language\n",
    "\n",
    "        Returns:\n",
    "            tgt_seq (list): list of tokens in target language as predicted\n",
    "            by the pre-trained model\n",
    "        \"\"\"\n",
    "        other = self.get_decoder_features(src_seq)\n",
    "\n",
    "        length = other['length'][0]\n",
    "\n",
    "        tgt_id_seq = [other['sequence'][di][0].data[0] for di in range(length)]\n",
    "        tgt_seq = [self.tgt_vocab.itos[tok] for tok in tgt_id_seq]\n",
    "        return tgt_seq\n",
    "\n",
    "    def predict_n(self, src_seq, n=1):\n",
    "        \"\"\" Make 'n' predictions given `src_seq` as input.\n",
    "\n",
    "        Args:\n",
    "            src_seq (list): list of tokens in source language\n",
    "            n (int): number of predicted seqs to return. If None,\n",
    "                     it will return just one seq.\n",
    "\n",
    "        Returns:\n",
    "            tgt_seq (list): list of tokens in target language as predicted\n",
    "                            by the pre-trained model\n",
    "        \"\"\"\n",
    "        other = self.get_decoder_features(src_seq)\n",
    "\n",
    "        result = []\n",
    "        for x in range(0, int(n)):\n",
    "            length = other['topk_length'][0][x]\n",
    "            tgt_id_seq = [other['topk_sequence'][di][0, x, 0].data[0] for di in range(length)]\n",
    "            tgt_seq = [self.tgt_vocab.itos[tok] for tok in tgt_id_seq]\n",
    "            result.append(tgt_seq)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicor = Predictor(my_seq2seq, input_vocab, output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/test_data.csv', encoding='utf-8')\n",
    "test_contents = list(test_data.content)\n",
    "# test_titles = list(test_data.title)[:20]\n",
    "# just use the former 20 sets as the \n",
    "test_contents =  [tokenizer(content) for content in test_contents]\n",
    "test_results = []\n",
    "for index, content_ in enumerate(test_contents):\n",
    "    test_title = ' '.join(predicor.predict(content_))\n",
    "    test_results.append(test_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, title_ in enumerate(test_results):\n",
    "    print(title_)\n",
    "    if index == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = [item[:-6] for item in test_results]\n",
    "for index, title_ in enumerate(test_results):\n",
    "    print(title_)\n",
    "    if index == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = '../data/result'\n",
    "if not os.path.exists(test_data_dir):\n",
    "    os.mkdir(test_data_dir)\n",
    "else:\n",
    "    shutil.rmtree(test_data_dir)\n",
    "    os.mkdir(test_data_dir)\n",
    "\n",
    "for i in range(len(test_results)):\n",
    "    with open(os.path.join(test_data_dir, str(i+1)+'.txt'), 'w') as f:\n",
    "        f.write(test_results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the unk and pad token in the \n",
    "display(content.vocab)\n",
    "display(title.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(title.vocab))  # the len method will drop the eos and sos token in the sentneces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
