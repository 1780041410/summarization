{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看数据的基本组织格式 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_dir = '../data/'\n",
    "base_names = ['bytecup.corpus.train.'+str(i) + '.txt' for i in range(9)]\n",
    "training_dataframe = [pd.read_json(os.path.join(data_dir, base_names[i]), lines=True) for i in range(9)]\n",
    "# training_dataframe = pd.concat(training_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>This was the year that felt like 50 years. We ...</td>\n",
       "      <td>21 Stories Our Readers Loved in 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Gary Vaynerchuk once told a 20 year old Taylor...</td>\n",
       "      <td>What To Do After Graduating College</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>On this episode of Recode Decode, hosted by Ka...</td>\n",
       "      <td>Full transcript: Former Groupon CEO Andrew Mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Today we are going to talk about the Albigensi...</td>\n",
       "      <td>The Albigensian Crusade And The Black Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>IntroductionThe more you want to impact the re...</td>\n",
       "      <td>The Surprising Secret To Changing The World</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            content  \\\n",
       "0   1  This was the year that felt like 50 years. We ...   \n",
       "1   2  Gary Vaynerchuk once told a 20 year old Taylor...   \n",
       "2   3  On this episode of Recode Decode, hosted by Ka...   \n",
       "3   4  Today we are going to talk about the Albigensi...   \n",
       "4   5  IntroductionThe more you want to impact the re...   \n",
       "\n",
       "                                               title  \n",
       "0               21 Stories Our Readers Loved in 2017  \n",
       "1                What To Do After Graduating College  \n",
       "2  Full transcript: Former Groupon CEO Andrew Mas...  \n",
       "3         The Albigensian Crusade And The Black Mass  \n",
       "4        The Surprising Secret To Changing The World  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_data = pd.read_csv(os.path.join(data_dir, 'train_data.csv'))\n",
    "# val_data = pd.read_csv(os.path.join(data_dir, 'val_data.csv'))\n",
    "# test_data = pd.read_csv(os.path.join(data_dir, 'test_data.csv'))\n",
    "# display(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(test_data)\n",
    "# # we will check the passag length and the title length, this \n",
    "# # will be the most fundamental features.\n",
    "\n",
    "# def vocab_length(item):\n",
    "#     new_item = item.split()\n",
    "#     return len(new_item)\n",
    "# test_data['title_length'] = test_data.title.apply(vocab_length)\n",
    "# test_data['content_length'] = test_data.content.apply(vocab_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA 第一部分，查看文章的长度和标题的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(test_data.head())\n",
    "# test_data.content_length.plot.hist(grid=True, bins=100, rwidth = 0.9, \n",
    "#                                            color = '#607103')\n",
    "# plt.title('the overall length of the training contents')\n",
    "# plt.xlabel('the length')\n",
    "# plt.ylabel('counts')\n",
    "# plt.grid(axis='y', alpha = 0.5)\n",
    "\n",
    "# plt.figure()\n",
    "# test_data.title_length.plot.hist(grid=True, bins=30, rwidth=0.9, color = \"#607123\")\n",
    "# plt.title(\"the overall length of the title\")\n",
    "# plt.xlabel(\"the length\")\n",
    "# plt.ylabel(\"the counts\")\n",
    "# plt.grid(axis='y', alpha=0.5)\n",
    "# display(test_data.content_length.mean())\n",
    "# display(test_data.content_length.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Full transcript: Former Groupon CEO Andrew Mason on Recode Decode'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_data.title)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1 大部分的文章都算是比较长的, 平均情况是大约为3000个单词，但是标题都是差不多是120个以内的。\n",
    "* 2 计算文章的tokenzier，使用nltk的sentence_tokenizer 来直接生成每一篇文章的content所划分的句子，以及每一个句子所对应的label，使得数据可以直接被SummrNN读取，之后再直接使用模块就可以了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the overall python rouge of each sentences\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def compute_rouge_l(r, p):\n",
    "    square_b = np.power(p/(r+1e-9) , 2)\n",
    "    l = (1+square_b)*(p*r)/(r+square_b*p)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a large part of their ritual became a mockery or parity of the catholic mass and became what has been called the black mass which they performed upon entry into their cult.at induction every novice had to denounce all catholic belief, spit upon the cross, renounce baptism and unction.\n",
      "the land was decimated.\n",
      "today we are going to talk about the albigensian crusade or the crusade of the church against the cathars.\n",
      "the albigensian crusade and the black mass\n",
      "CPU times: user 68 ms, sys: 4 ms, total: 72 ms\n",
      "Wall time: 69.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "idx = 3\n",
    "rouge = Rouge()\n",
    "context_sentences = tokenizer.sentences_from_text(train_data.content[idx].lower())\n",
    "# print(context_sentences)\n",
    "# print(context_sentences)\n",
    "reference = train_data.title[idx].lower()\n",
    "# # for test\n",
    "precession_result = []\n",
    "recall_result=[]\n",
    "f1_result = []\n",
    "for index, summary in enumerate(context_sentences):\n",
    "    score = rouge.get_scores(reference, summary)[0]['rouge-l']\n",
    "    # for the precession index and the recall index, note that the common \n",
    "    # word in the f1-index and the recall-index will must be the same appeared\n",
    "    # in the title here.\n",
    "    precession_result.append(score['p'])\n",
    "    recall_result.append(score['r'])\n",
    "    f1_result.append(score['f'])\n",
    "precession_index = np.argsort(-1*np.array(precession_result))[0]\n",
    "recall_index = np.argsort(-1*np.array(recall_result))[0]\n",
    "f1_index = np.argsort(-1*np.array(f1_result))[0]\n",
    "print(context_sentences[precession_index])\n",
    "print(context_sentences[recall_index])\n",
    "print(context_sentences[f1_index])\n",
    "print(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "here, we are going to define a function, which will extract the key information in the\n",
    "context, a tokenizer will split the data to a bunch of sentences, and later, the sentences\n",
    "will also need to be labeled according to the precession and recall to the title and also\n",
    "the f-1 score. becasue the label will be from the title, so soleyly rely on the label will \n",
    "become useless, must based on all the data of the document.\n",
    "\"\"\"\n",
    "def filter_doc(content):\n",
    "    \"\"\"\n",
    "    filter some word in content, here, main feature is to filter thr original\n",
    "    content and return the standard content list.\n",
    "    \"\"\"\n",
    "    content = content.lower().strip('.\\n')\n",
    "\n",
    "\n",
    "def create_label(content, title):\n",
    "    \"\"\"\n",
    "    content: the basic contents of the data\n",
    "    title: the article title of the data\n",
    "    tokenzier: to split the content to a list of\n",
    "    sententces.\n",
    "    \n",
    "    return: a list of binary sequence, 1 indicate the sentence will\n",
    "    be choosen and 0 otherwise.\n",
    "    \"\"\"\n",
    "    content = content.lower().replace('.', '. ')\n",
    "    content = content.replace('...', 'some_unk_tokens')\n",
    "    context_sentences = tokenizer.sentences_from_text(content)\n",
    "    context_sentences = [item for item in context_sentences if len(item) >= 2]\n",
    "    title = title.lower()\n",
    "    if len(title)  <= 1:\n",
    "        title = 'the ' + title\n",
    "    score = []\n",
    "    for summary in context_sentences:\n",
    "#         summary = summary.lower()\n",
    "#         if len(summary) <= 1 or summary == '...':\n",
    "#             summary = 'some_unk_toknes'\n",
    "        try:\n",
    "            score.append(list(rouge.get_scores(title, summary)[0]['rouge-l'].values()))\n",
    "        except ValueError:\n",
    "            print(\"++++++++++++++++++++++\")\n",
    "            print(title)\n",
    "            print(\"+++++++++++++++++++++\")\n",
    "            print(summary)\n",
    "    # the first col is f, the second col is p, and the later one\n",
    "    # is r\n",
    "    max_f_index, max_p_index, max_r_index = np.argmax(np.stack(score, axis=0), axis=0)\n",
    "    label = np.zeros(len(context_sentences))\n",
    "    label[max_f_index] = 1\n",
    "    label[max_p_index] = 1\n",
    "    label[max_r_index] = 1\n",
    "    label = label.astype('int32')\n",
    "    return '\\n'.join(str(item) for item in label), '\\n'.join(context_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3116\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3191\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3192\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3391\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3392\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3393\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3394\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3395\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36m_asarray_tuplesafe\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconstruct_1d_object_array_from_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for index, train_data in enumerate(training_dataframe):\n",
    "    train_data['labels'], train_data['sentences'] = zip(*train_data.apply(lambda x:create_label(x['content'], x['title']), axis=1))\n",
    "    training_dataframe[index] = train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write the data to json file, using dataframe.to_json(), notice use head = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we ../data/o save the data to two parts, train.json, val.json and test.json.\n",
    "with open('../data/train.json', 'w') as f:\n",
    "    f.write(save_data.to_json(orient='records', lines=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
