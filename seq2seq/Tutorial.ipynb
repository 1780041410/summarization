{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenxiang/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "import torchtext\n",
    "from IPython.display import display\n",
    "from trainer import SupervisedTrainer\n",
    "from models import EncoderRNN, DecoderRNN, Seq2seq\n",
    "from dataset import SourceField, TargetField\n",
    "from optim import Optimizer\n",
    "from loss import Perplexity\n",
    "from evaluator import Predictor\n",
    "from torchtext.data import Field\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from torchtext.data import TabularDataset\n",
    "from util.checkpoint import Checkpoint\n",
    "import csv\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was the year that felt like 50 years. We ...</td>\n",
       "      <td>21 Stories Our Readers Loved in 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gary Vaynerchuk once told a 20 year old Taylor...</td>\n",
       "      <td>What To Do After Graduating College</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  This was the year that felt like 50 years. We ...   \n",
       "1  Gary Vaynerchuk once told a 20 year old Taylor...   \n",
       "\n",
       "                                  title  id  \n",
       "0  21 Stories Our Readers Loved in 2017   0  \n",
       "1   What To Do After Graduating College   1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../data/'\n",
    "file_name = 'train_data.csv'\n",
    "dev_name = 'val_data.csv'\n",
    "train_data = pd.read_csv(os.path.join(data_dir, file_name), encoding='utf-8')\n",
    "display(train_data.head(n=2))\n",
    "csv.field_size_limit(100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentences):\n",
    "    sentences = sentences.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentences)\n",
    "    filtered_words = [w for w in tokens]\n",
    "    return filtered_words\n",
    "\n",
    "max_encoder_len = 800\n",
    "min_decoder_len = 1\n",
    "content, title = SourceField(tokenize=tokenizer), TargetField(tokenize=tokenizer)\n",
    "def len_filter(example):\n",
    "    return len(example.content) <= max_encoder_len and len(example.title) >= min_decoder_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 31s, sys: 10.5 s, total: 2min 41s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tv_datafields = [('content', content), ('title', title), ('id', None)]  # must order the data format with the csv file.\n",
    "trn = TabularDataset(path=os.path.join(data_dir, file_name), \n",
    "                     format='csv', fields=tv_datafields, skip_header=True,\n",
    "                     filter_pred=len_filter)\n",
    "\n",
    "dev = TabularDataset(path=os.path.join(data_dir, dev_name),\n",
    "                    format='csv', fields = tv_datafields, skip_header=True,\n",
    "                    filter_pred=len_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.build_vocab(trn, max_size = 50000)\n",
    "title.build_vocab(trn, max_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 7215050),\n",
       " ('to', 3798408),\n",
       " ('and', 3324064),\n",
       " ('a', 3130495),\n",
       " ('of', 2999731),\n",
       " ('in', 2567508),\n",
       " ('s', 1543622),\n",
       " ('that', 1397337),\n",
       " ('for', 1383377),\n",
       " ('is', 1297595)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('<sos>', 322358),\n",
       " ('<eos>', 322358),\n",
       " ('to', 93079),\n",
       " ('the', 71872),\n",
       " ('in', 64768),\n",
       " ('s', 61315),\n",
       " ('of', 51853),\n",
       " ('for', 47829),\n",
       " ('a', 40793),\n",
       " ('and', 36461)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(content.vocab.freqs.most_common(10))\n",
    "display(title.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab = content.vocab\n",
    "output_vocab = title.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenxiang/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# build the model here.\n",
    "weight = torch.ones(len(title.vocab))\n",
    "pad = title.vocab.stoi[title.pad_token]\n",
    "loss = Perplexity(weight, pad)\n",
    "loss.cuda()\n",
    "seq2seq = None\n",
    "optimizer = None\n",
    "hidden_size = 100\n",
    "bidirectional = True\n",
    "# add the pretrained embedding here\n",
    "# encoder_embedding = torch.from_numpy(np.load('../data/encoder_embedding_50000_100.npy'))\n",
    "# decoder_embedding = torch.from_numpy(np.load('../data/decoder_embedding_50000_100.npy'))\n",
    "# display(encoder_embedding.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(len(content.vocab), max_encoder_len, hidden_size, bidirectional=bidirectional, dropout_p=0.2, n_layers=2,\n",
    "                     variable_lengths=True, update_embedding=True)\n",
    "decoder = DecoderRNN(len(title.vocab), 20, hidden_size*2 if bidirectional else hidden_size, dropout_p=0.2, n_layers=2, use_attention=True, \n",
    "                     bidirectional=bidirectional, eos_id = title.eos_id, sos_id = title.sos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2seq(\n",
       "  (encoder): EncoderRNN(\n",
       "    (input_dropout): Dropout(p=0)\n",
       "    (embedding): Embedding(50002, 100)\n",
       "    (rnn): GRU(100, 100, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  )\n",
       "  (decoder): DecoderRNN(\n",
       "    (input_dropout): Dropout(p=0)\n",
       "    (rnn): GRU(200, 200, num_layers=2, batch_first=True, dropout=0.2)\n",
       "    (embedding): Embedding(50002, 200)\n",
       "    (attention): Attention(\n",
       "      (linear_out): Linear(in_features=400, out_features=200, bias=True)\n",
       "    )\n",
       "    (out): Linear(in_features=200, out_features=50002, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_seq2seq =Seq2seq(encoder, decoder)\n",
    "my_seq2seq.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in my_seq2seq.parameters():\n",
    "    param.data.uniform_(-0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = SupervisedTrainer(loss = loss, batch_size=40, checkpoint_every=3e4, print_every=6e2, expt_dir='../data', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 3%, Train Perplexity: 107.9859\n",
      "Progress: 5%, Train Perplexity: 60.6364\n",
      "Progress: 7%, Train Perplexity: 61.6174\n",
      "Progress: 9%, Train Perplexity: 61.7865\n",
      "Progress: 11%, Train Perplexity: 65.2091\n",
      "Progress: 13%, Train Perplexity: 64.2776\n",
      "Progress: 14%, Train Perplexity: 62.4434\n",
      "Progress: 16%, Train Perplexity: 64.6698\n",
      "Progress: 18%, Train Perplexity: 65.5042\n",
      "Progress: 20%, Train Perplexity: 64.4328\n",
      "Progress: 22%, Train Perplexity: 65.2106\n",
      "Progress: 24%, Train Perplexity: 65.5941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenxiang/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train), lengths\n",
      "/home/chenxiang/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1: Train Perplexity: 62.4367, Dev Perplexity: 522.2506, Accuracy: 0.1508\n",
      "Progress: 26%, Train Perplexity: 54.5309\n",
      "Progress: 27%, Train Perplexity: 46.1106\n",
      "Progress: 29%, Train Perplexity: 46.8940\n",
      "Progress: 31%, Train Perplexity: 49.2396\n",
      "Progress: 33%, Train Perplexity: 50.0597\n",
      "Progress: 35%, Train Perplexity: 48.6186\n",
      "Progress: 37%, Train Perplexity: 53.5088\n",
      "Progress: 39%, Train Perplexity: 54.0555\n",
      "Progress: 40%, Train Perplexity: 52.5691\n",
      "Progress: 42%, Train Perplexity: 56.2621\n",
      "Progress: 44%, Train Perplexity: 54.5354\n",
      "Progress: 46%, Train Perplexity: 56.5594\n",
      "Progress: 48%, Train Perplexity: 56.0629\n",
      "Finished epoch 2: Train Perplexity: 51.9704, Dev Perplexity: 530.9379, Accuracy: 0.1518\n",
      "Progress: 50%, Train Perplexity: 54.4689\n",
      "Progress: 52%, Train Perplexity: 40.1282\n",
      "Progress: 53%, Train Perplexity: 39.7388\n",
      "Progress: 55%, Train Perplexity: 43.3713\n",
      "Progress: 57%, Train Perplexity: 44.5111\n",
      "Progress: 59%, Train Perplexity: 46.4257\n",
      "Progress: 61%, Train Perplexity: 45.8141\n",
      "Progress: 63%, Train Perplexity: 45.9773\n",
      "Progress: 65%, Train Perplexity: 50.4815\n",
      "Progress: 67%, Train Perplexity: 48.2264\n",
      "Progress: 68%, Train Perplexity: 52.9173\n",
      "Progress: 70%, Train Perplexity: 49.8417\n",
      "Progress: 72%, Train Perplexity: 52.9440\n",
      "Progress: 74%, Train Perplexity: 52.6808\n",
      "Finished epoch 3: Train Perplexity: 47.1736, Dev Perplexity: 551.7041, Accuracy: 0.1528\n",
      "Progress: 76%, Train Perplexity: 40.4159\n",
      "Progress: 78%, Train Perplexity: 36.8822\n",
      "Progress: 80%, Train Perplexity: 39.3633\n",
      "Progress: 81%, Train Perplexity: 40.6312\n",
      "Progress: 83%, Train Perplexity: 40.9980\n",
      "Progress: 85%, Train Perplexity: 41.8715\n",
      "Progress: 87%, Train Perplexity: 42.4642\n",
      "Progress: 89%, Train Perplexity: 44.9161\n",
      "Progress: 91%, Train Perplexity: 46.3848\n",
      "Progress: 93%, Train Perplexity: 46.8088\n",
      "Progress: 94%, Train Perplexity: 47.9896\n",
      "Progress: 96%, Train Perplexity: 47.5234\n",
      "Progress: 98%, Train Perplexity: 48.4113\n",
      "Finished epoch 4: Train Perplexity: 43.6012, Dev Perplexity: 587.4699, Accuracy: 0.1499\n"
     ]
    }
   ],
   "source": [
    "my_seq2seq = t.train(my_seq2seq, trn, num_epochs=4, dev_data=dev, optimizer=optimizer, teacher_forcing_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Predictor(object):\n",
    "\n",
    "    def __init__(self, model, src_vocab, tgt_vocab):\n",
    "        \"\"\"\n",
    "        Predictor class to evaluate for a given model.\n",
    "        Args:\n",
    "            model (seq2seq.models): trained model. This can be loaded from a checkpoint\n",
    "                using `seq2seq.util.checkpoint.load`\n",
    "            src_vocab (seq2seq.dataset.vocabulary.Vocabulary): source sequence vocabulary\n",
    "            tgt_vocab (seq2seq.dataset.vocabulary.Vocabulary): target sequence vocabulary\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = model.cuda()\n",
    "        else:\n",
    "            self.model = model.cpu()\n",
    "        self.model.eval()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def get_decoder_features(self, src_seq):\n",
    "        src_id_seq = torch.LongTensor([self.src_vocab.stoi[tok] for tok in src_seq]).view(1, -1)\n",
    "        if torch.cuda.is_available():\n",
    "            src_id_seq = src_id_seq.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            softmax_list, _, other = self.model(src_id_seq, [len(src_seq)])\n",
    "\n",
    "        return other\n",
    "\n",
    "    def predict(self, src_seq):\n",
    "        \"\"\" Make prediction given `src_seq` as input.\n",
    "\n",
    "        Args:\n",
    "            src_seq (list): list of tokens in source language\n",
    "\n",
    "        Returns:\n",
    "            tgt_seq (list): list of tokens in target language as predicted\n",
    "            by the pre-trained model\n",
    "        \"\"\"\n",
    "        other = self.get_decoder_features(src_seq)\n",
    "\n",
    "        length = other['length'][0]\n",
    "\n",
    "        tgt_id_seq = [other['sequence'][di][0].data[0] for di in range(length)]\n",
    "        tgt_seq = [self.tgt_vocab.itos[tok] for tok in tgt_id_seq]\n",
    "        return tgt_seq\n",
    "\n",
    "    def predict_n(self, src_seq, n=1):\n",
    "        \"\"\" Make 'n' predictions given `src_seq` as input.\n",
    "\n",
    "        Args:\n",
    "            src_seq (list): list of tokens in source language\n",
    "            n (int): number of predicted seqs to return. If None,\n",
    "                     it will return just one seq.\n",
    "\n",
    "        Returns:\n",
    "            tgt_seq (list): list of tokens in target language as predicted\n",
    "                            by the pre-trained model\n",
    "        \"\"\"\n",
    "        other = self.get_decoder_features(src_seq)\n",
    "\n",
    "        result = []\n",
    "        for x in range(0, int(n)):\n",
    "            length = other['topk_length'][0][x]\n",
    "            tgt_id_seq = [other['topk_sequence'][di][0, x, 0].data[0] for di in range(length)]\n",
    "            tgt_seq = [self.tgt_vocab.itos[tok] for tok in tgt_id_seq]\n",
    "            result.append(tgt_seq)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicor = Predictor(my_seq2seq, input_vocab, output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/test_data.csv', encoding='utf-8')\n",
    "test_contents = list(test_data.content)\n",
    "# test_titles = list(test_data.title)[:20]\n",
    "# just use the former 20 sets as the \n",
    "test_contents =  [tokenizer(content) for content in test_contents]\n",
    "test_results = []\n",
    "for index, content_ in enumerate(test_contents):\n",
    "    test_title = ' '.join(predicor.predict(content_))\n",
    "    test_results.append(test_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how a mom is murder in a crime <eos>\n",
      "liverpool fans have liverpool ever ever liverpool <eos>\n",
      "these people are people are to to <eos>\n",
      "you are the most you you <eos>\n",
      "the are are the <eos>\n",
      "what 5 things you should know about <eos>\n",
      "japanese s abe is a s s s s s in <eos>\n",
      "amazon s new is new with with with <eos>\n",
      "there s a flight for the delta and to <eos>\n",
      "23 years now now you get free serving you <eos>\n",
      "gordon ramsay is vegan vegan vegan to to to <eos>\n",
      "krispy kreme is kreme kreme kreme kreme kreme kreme kreme <eos>\n",
      "get a out sized bag packs 20 percent of a m s <eos>\n",
      "internet starbucks launches starbucks starbucks clothing store <eos>\n",
      "amazon s amazon to a delivery <eos>\n",
      "<unk> 5 5 to <eos>\n",
      "the 18 day of <eos>\n",
      "khloé kardashian s her khloé her her her her her <eos>\n",
      "the best spot for the weirdest camera videos <eos>\n",
      "the funniest craziest cats you hair in your red cats <eos>\n",
      "the dog dog dog dog dog dog with <eos>\n"
     ]
    }
   ],
   "source": [
    "for index, title_ in enumerate(test_results):\n",
    "    print(title_)\n",
    "    if index == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how a mom is murder in a crime\n",
      "liverpool fans have liverpool ever ever liverpool\n",
      "these people are people are to to\n",
      "you are the most you you\n",
      "the are are the\n",
      "what 5 things you should know about\n",
      "japanese s abe is a s s s s s in\n",
      "amazon s new is new with with with\n",
      "there s a flight for the delta and to\n",
      "23 years now now you get free serving you\n",
      "gordon ramsay is vegan vegan vegan to to to\n",
      "krispy kreme is kreme kreme kreme kreme kreme kreme kreme\n",
      "get a out sized bag packs 20 percent of a m s\n",
      "internet starbucks launches starbucks starbucks clothing store\n",
      "amazon s amazon to a delivery\n",
      "<unk> 5 5 to\n",
      "the 18 day of\n",
      "khloé kardashian s her khloé her her her her her\n",
      "the best spot for the weirdest camera videos\n",
      "the funniest craziest cats you hair in your red cats\n",
      "the dog dog dog dog dog dog with\n"
     ]
    }
   ],
   "source": [
    "test_results = [item[:-6] for item in test_results]\n",
    "for index, title_ in enumerate(test_results):\n",
    "    print(title_)\n",
    "    if index == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = '../data/result'\n",
    "if not os.path.exists(test_data_dir):\n",
    "    os.mkdir(test_data_dir)\n",
    "else:\n",
    "    shutil.rmtree(test_data_dir)\n",
    "    os.mkdir(test_data_dir)\n",
    "\n",
    "for i in range(len(test_results)):\n",
    "    with open(os.path.join(test_data_dir, str(i+1)+'.txt'), 'w') as f:\n",
    "        f.write(test_results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x7f92fbe29cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x7f92fbe29cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to check the unk and pad token in the \n",
    "display(content.vocab)\n",
    "display(title.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(title.vocab))  # the len method will drop the eos and sos token in the sentneces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrainde model here to initialize the embedding matrix here.\n",
    "embedding_matrix = np.zeros((len(content.vocab), 100))\n",
    "word_to_vec_path = '../data/glove.6B.100d.txt'\n",
    "def get_eng_vec(path= word_to_vec_path):\n",
    "    word_to_vec = dict()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line=line.split(' ')\n",
    "            word_to_vec[line[0]]= [float(f) for f in line[1:]]\n",
    "    return word_to_vec\n",
    "\n",
    "word_to_vec = get_eng_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find unknow 0 words in word2vec\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.random.randn(len(title.vocab), 100)\n",
    "for index in range(embedding_matrix.shape[0]):\n",
    "    unknow_words = 0\n",
    "    word = title.vocab.itos[index]\n",
    "    try: # try to find the word in the word_to_vec:\n",
    "        vector = word_to_vec[word]\n",
    "        embedding_matrix[index] = vector\n",
    "    except KeyError:\n",
    "        unknow_words += 1\n",
    "        pass\n",
    "print(\"find unknow {} words in word2vec\".format(unknow_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[-0.1897,\n",
       " 0.050024,\n",
       " 0.19084,\n",
       " -0.049184,\n",
       " -0.089737,\n",
       " 0.21006,\n",
       " -0.54952,\n",
       " 0.098377,\n",
       " -0.20135,\n",
       " 0.34241,\n",
       " -0.092677,\n",
       " 0.161,\n",
       " -0.13268,\n",
       " -0.2816,\n",
       " 0.18737,\n",
       " -0.42959,\n",
       " 0.96039,\n",
       " 0.13972,\n",
       " -1.0781,\n",
       " 0.40518,\n",
       " 0.50539,\n",
       " -0.55064,\n",
       " 0.4844,\n",
       " 0.38044,\n",
       " -0.0029055,\n",
       " -0.34942,\n",
       " -0.099696,\n",
       " -0.78368,\n",
       " 1.0363,\n",
       " -0.2314,\n",
       " -0.47121,\n",
       " 0.57126,\n",
       " -0.21454,\n",
       " 0.35958,\n",
       " -0.48319,\n",
       " 1.0875,\n",
       " 0.28524,\n",
       " 0.12447,\n",
       " -0.039248,\n",
       " -0.076732,\n",
       " -0.76343,\n",
       " -0.32409,\n",
       " -0.5749,\n",
       " -1.0893,\n",
       " -0.41811,\n",
       " 0.4512,\n",
       " 0.12112,\n",
       " -0.51367,\n",
       " -0.13349,\n",
       " -1.1378,\n",
       " -0.28768,\n",
       " 0.16774,\n",
       " 0.55804,\n",
       " 1.5387,\n",
       " 0.018859,\n",
       " -2.9721,\n",
       " -0.24216,\n",
       " -0.92495,\n",
       " 2.1992,\n",
       " 0.28234,\n",
       " -0.3478,\n",
       " 0.51621,\n",
       " -0.43387,\n",
       " 0.36852,\n",
       " 0.74573,\n",
       " 0.072102,\n",
       " 0.27931,\n",
       " 0.92569,\n",
       " -0.050336,\n",
       " -0.85856,\n",
       " -0.1358,\n",
       " -0.92551,\n",
       " -0.33991,\n",
       " -1.0394,\n",
       " -0.067203,\n",
       " -0.21379,\n",
       " -0.4769,\n",
       " 0.21377,\n",
       " -0.84008,\n",
       " 0.052536,\n",
       " 0.59298,\n",
       " 0.29604,\n",
       " -0.67644,\n",
       " 0.13916,\n",
       " -1.5504,\n",
       " -0.20765,\n",
       " 0.7222,\n",
       " 0.52056,\n",
       " -0.076221,\n",
       " -0.15194,\n",
       " -0.13134,\n",
       " 0.058617,\n",
       " -0.31869,\n",
       " -0.61419,\n",
       " -0.62393,\n",
       " -0.41548,\n",
       " -0.038175,\n",
       " -0.39804,\n",
       " 0.47647,\n",
       " -0.15983]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([-1.8970e-01,  5.0024e-02,  1.9084e-01, -4.9184e-02, -8.9737e-02,\n",
       "        2.1006e-01, -5.4952e-01,  9.8377e-02, -2.0135e-01,  3.4241e-01,\n",
       "       -9.2677e-02,  1.6100e-01, -1.3268e-01, -2.8160e-01,  1.8737e-01,\n",
       "       -4.2959e-01,  9.6039e-01,  1.3972e-01, -1.0781e+00,  4.0518e-01,\n",
       "        5.0539e-01, -5.5064e-01,  4.8440e-01,  3.8044e-01, -2.9055e-03,\n",
       "       -3.4942e-01, -9.9696e-02, -7.8368e-01,  1.0363e+00, -2.3140e-01,\n",
       "       -4.7121e-01,  5.7126e-01, -2.1454e-01,  3.5958e-01, -4.8319e-01,\n",
       "        1.0875e+00,  2.8524e-01,  1.2447e-01, -3.9248e-02, -7.6732e-02,\n",
       "       -7.6343e-01, -3.2409e-01, -5.7490e-01, -1.0893e+00, -4.1811e-01,\n",
       "        4.5120e-01,  1.2112e-01, -5.1367e-01, -1.3349e-01, -1.1378e+00,\n",
       "       -2.8768e-01,  1.6774e-01,  5.5804e-01,  1.5387e+00,  1.8859e-02,\n",
       "       -2.9721e+00, -2.4216e-01, -9.2495e-01,  2.1992e+00,  2.8234e-01,\n",
       "       -3.4780e-01,  5.1621e-01, -4.3387e-01,  3.6852e-01,  7.4573e-01,\n",
       "        7.2102e-02,  2.7931e-01,  9.2569e-01, -5.0336e-02, -8.5856e-01,\n",
       "       -1.3580e-01, -9.2551e-01, -3.3991e-01, -1.0394e+00, -6.7203e-02,\n",
       "       -2.1379e-01, -4.7690e-01,  2.1377e-01, -8.4008e-01,  5.2536e-02,\n",
       "        5.9298e-01,  2.9604e-01, -6.7644e-01,  1.3916e-01, -1.5504e+00,\n",
       "       -2.0765e-01,  7.2220e-01,  5.2056e-01, -7.6221e-02, -1.5194e-01,\n",
       "       -1.3134e-01,  5.8617e-02, -3.1869e-01, -6.1419e-01, -6.2393e-01,\n",
       "       -4.1548e-01, -3.8175e-02, -3.9804e-01,  4.7647e-01, -1.5983e-01])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(title.vocab.stoi['to'])\n",
    "vector = word_to_vec['to']\n",
    "display(vector)\n",
    "display(embedding_matrix[4,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/decoder_embedding_50000_100.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_embedding = torch.from_numpy(embedding_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
